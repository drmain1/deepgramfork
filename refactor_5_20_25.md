### Deepgram Integration Refactoring Summary (May 20, 2025)

The primary goal of this refactoring was to modularize the Deepgram streaming and audio processing logic, moving it out of the main application file (`backend/main.py`) into a dedicated utility module (`backend/deepgram_utils.py`). This enhances code organization, maintainability, and readability.

**Key Actions Performed:**

1.  **Created `backend/deepgram_utils.py` Module:**
    *   A new file, `backend/deepgram_utils.py`, was created to house all Deepgram-specific functionalities.
    *   **`handle_deepgram_websocket` function:** This central async function was implemented in `deepgram_utils.py`. It now manages the entire lifecycle of a client's WebSocket connection for Deepgram streaming, including:
        *   Accepting WebSocket connections.
        *   Receiving initial client messages (containing `user_id` and `profile_id`).
        *   Fetching user-specific transcription profiles using a callback function (`get_user_settings_func` passed from `main.py`).
        *   Initializing the Deepgram `AsyncLiveClient` with appropriate options derived from user profiles (e.g., `smart_format`, `diarize`, `utterance_end_ms`, `vad_events`).
        *   Setting up Deepgram event handlers (e.g., `on_open`, `on_message`, `on_error`, `on_close`).
        *   Managing an FFmpeg subprocess for real-time transcoding of incoming client audio (e.g., Opus) to raw PCM (s16le, 16kHz, mono).
        *   Streaming the transcoded PCM audio to Deepgram.
        *   Simultaneously writing the PCM audio to a temporary `.pcm` file.
        *   Sending interim and final transcripts back to the client via WebSocket.
        *   Handling WebSocket disconnections and errors gracefully.
        *   Performing cleanup operations, including closing the Deepgram connection and terminating the FFmpeg process.
    *   **`convert_raw_pcm_to_wav` function:** This async function (previously in `main.py`) was moved to `deepgram_utils.py`. It converts the temporary `.pcm` file (containing the full session's audio) into a `.wav` file using FFmpeg after the streaming session ends.
    *   **Temporary Audio Directory:** A specific temporary directory `DG_TEMP_AUDIO_DIR` was established within `deepgram_utils.py` for managing the intermediate `.pcm` and final `.wav` files generated by this module.
    *   **Logging:** Comprehensive logging was maintained and configured within this module.

2.  **Refactored `backend/main.py`:**
    *   **Simplified `websocket_stream_endpoint`:** The existing `@app.websocket("/stream")` endpoint in `main.py` was significantly refactored. Its entire body was replaced with a single call:
        ```python
        await handle_deepgram_websocket(websocket, get_user_settings)
        ```
        This delegates all complex streaming logic to the new utility module. The `get_user_settings` function (already present in `main.py` for fetching user settings from S3) is passed as a callback.
    *   **Added Import:** The necessary import statement was added:
        ```python
        from backend.deepgram_utils import handle_deepgram_websocket
        ```
    *   **Removed Redundant Code:** The local `convert_raw_pcm_to_wav` function and extensive Deepgram-related logic (event handlers, FFmpeg management, etc.) were removed from `main.py` as they are now handled by `deepgram_utils.py`.

3.  **Code and Configuration Adjustments:**
    *   **FFmpeg Usage:** Confirmed that FFmpeg is invoked directly via `asyncio.create_subprocess_exec` for both real-time transcoding and final WAV conversion. An unused `import ffmpeg` (from the `ffmpeg-python` wrapper library, which wasn't being used for command construction) was removed from `deepgram_utils.py`.
    *   **User Settings Application:** Ensured that user-defined transcription profile settings (e.g., `smart_format`, `diarize`, `utterances`) fetched via `get_user_settings` are correctly applied to Deepgram's `LiveOptions` in `handle_deepgram_websocket`. The `utterances` boolean from the profile correctly controls `utterance_end_ms` and `vad_events`.

**Outcome:**

The refactoring has resulted in a cleaner separation of concerns. `main.py` is now primarily responsible for FastAPI routing, application setup, and high-level request handling, while `deepgram_utils.py` cohesively manages all aspects of the Deepgram audio streaming and processing pipeline. This should make future development and debugging more straightforward.

### Post-Refactoring Adjustments and Fixes

After the initial refactoring, several adjustments were made to ensure the application runs correctly and to fix the WebSocket message incompatibility with the frontend:

1.  **Python Import and Execution Adjustments:**
    *   **Relative Imports:** In `backend/main.py`, absolute imports for modules within the `backend` package were changed to relative imports to resolve `ModuleNotFoundError` issues when running `main.py` as a script.
        *   `from backend.deepgram_utils import handle_deepgram_websocket` became `from .deepgram_utils import handle_deepgram_websocket`.
        *   `from backend.aws_utils import ...` became `from .aws_utils import ...`.
    *   **`__init__.py`:** An empty `backend/__init__.py` file was created to ensure the `backend` directory is treated as a Python package, which is crucial for relative imports to function correctly.
    *   **Uvicorn Run Command:** The `uvicorn.run()` call within the `if __name__ == "__main__":` block in `backend/main.py` was updated from `uvicorn.run("main:app", ...)` to `uvicorn.run("backend.main:app", ...)` to correctly specify the application path when `main.py` is run as a script from the project root.

2.  **WebSocket Message Format Alignment (`backend/deepgram_utils.py`):**
    *   The `on_message_handler` (nested within `handle_deepgram_websocket`) was modified to change the structure of messages sent to the client for transcript updates.
    *   Previously, messages were sent with types like `interim_transcript` or `final_transcript`.
    *   This was changed to send a consistent `type: "transcript"`, with the transcript content under a `"text"` key, and an added boolean field `"is_final"` to distinguish between interim and final updates. The `session_id` was retained.
    *   This change ensures that the backend's WebSocket messages match the format expected by the frontend's `AudioRecorder.jsx` component, allowing the UI to correctly display streaming transcription.

**Overall Impact of Adjustments:**

These follow-up changes were critical for:
*   Ensuring the backend server could be started reliably using `python backend/main.py` from the project root.
*   Resolving the primary issue where the frontend was not displaying streaming transcripts due to a mismatch in WebSocket message formats.
*   The application's streaming transcription functionality is now confirmed to be working as expected.